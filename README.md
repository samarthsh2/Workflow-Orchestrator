# Workflow-Orchestrator
A Workflow Orchestrator using Databricks API.

## Table of Contents

1. [Introduction](#introduction)
2. [Folder definations and it's contents](#folder-definations-and-its-contents)
3. [Guide to use the System](#guide-to-use-the-system)
4. [References](#references)


## Introduction

This system is designed to automate the creation of workflows that generate tasks and allocate the specified compute resources for their completion. The process is fully automated; users simply need to configure the compute settings and add the notebooks or files for which tasks need to be created.


## Folder definations and it's contents

In this section, I will explain the purpose of storing contents in the designated folders and outline the systemâ€™s file structure.

> |___compute\
> &nbsp;&nbsp;&nbsp;&nbsp;|__cluster_compute.py\
> &nbsp;&nbsp;&nbsp;&nbsp;|__warehouse_compute.py\
> |__config\
> &nbsp;&nbsp;&nbsp;&nbsp;|__cluster_config.json\
> &nbsp;&nbsp;&nbsp;&nbsp;|__warehouse_config.json\
> |__notebooks\
> |__Task_dependecy\
> &nbsp;&nbsp;&nbsp;&nbsp;|__dependecy.py\
> |__jobs.py\
> |__tasks.py\
> |__workflow.ipynb

The contents of each folder or file are described below:
> i. **_compute_**
>
>> Contains the logic for creating, updating, and restarting compute resources.
>
> ii. **_config_**
>
>> Holds the JSON files for cluster and warehouse configurations and their names.
>
> iii. **_notebooks_**
>
>> Contains the main code for creating pipelines.
>
> iv. **_Tasks_dependency_**
>
>> Includes a file named _dependency.py_, which lists the dependencies each task has on other tasks, including cases with no dependencies.
>
> v. **_jobs.py_**
>
>> Contains the code for creating jobs using tasks generated by the _tasks.py_ file.
>
> vi. **_tasks.py_**
>
>> Creates tasks for the notebooks and establishes dependencies on other notebooks, referencing _dependency.py_ for dependency information.
>
> vii. **_workflow.ipynb_**
>
>> Contains initialization code for the created job.
>
>
> You can update the code as needed to implement custom functionality


## Guide to use the System

To use this system, follow these steps for a seamless workflow setup:
> **Update Configuration Files:**
> - Navigate to the _config_ folder and update the configuration files as needed.\
> - **Note:** If you don't require a cluster or warehouse, do not delete the file. Instead, remove the respective configuration from the file.
> - Use the following syntax for storing compute configurations:
>     ```json
>     {
>         "cluster-name": {"cluster-configuration"}
>     }
>     ```
> 
> - For assistance with cluster configuration, refer to the Databricks API documentation mentioned in the References Section.
> 
> **Add Files to Notebooks Folder:** 
> - Once the configuration is updated in the _config_ folder, add the files needed to build the required job and tasks in the _notebooks_ folder.
>
> **Update Task Dependencies:**
> - After uploading the code files to the _notebooks_ folder, add task dependencies and specify the compute resources to be used. Update the _dependency.py_ file in the _Tasks_Dependecy_ folder using the following syntax:
>   ```json
>   {
>       "file_name": "file-name",
>       "format": "format-of-file",
>       "dependent_on": ["tasks-names-on-which-the-current-task-is-dependent-on"],
>       "compute_name": "compute-name"
>   }
>   ```
> - **Note:** If the code file is a notebook, use **_py_** as the file format. It is also recommended to name tasks the same as the file name for better clarity.
>
> **Create and Run Job:**
> - Go to the _workflow.ipynb_ file, update the credentials, and click on run to create a job.
> - **Note:** The job will not start automatically; you will need to trigger it manually or write separate logic for automatic triggering.


## References

- [Databricks API documentation]('https://docs.databricks.com/api/azure/workspace/introduction')
